{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94594b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import cv2\n",
    "from skimage import feature\n",
    "from skimage.filters import gabor\n",
    "from skimage.color import rgb2gray\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "# Path dataset\n",
    "base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "train_path = os.path.join(base_dir, \"train\")\n",
    "test_path = os.path.join(base_dir, \"test\")\n",
    "output_dir = os.path.join(base_dir, \"output\")\n",
    "\n",
    "print(\"Path to train dataset:\", train_path)\n",
    "print(\"Path to test dataset:\", test_path)\n",
    "print(\"Path to output directory:\", output_dir)\n",
    "\n",
    "# Konstanta\n",
    "IMG_SIZE = 224\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Kelas untuk ekstraksi fitur tekstur\n",
    "class TextureFeatureExtractor:\n",
    "    \"\"\"Ekstraksi fitur tekstur untuk klasifikasi kain\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract_glcm_features(self, image):\n",
    "        gray = rgb2gray(image) if len(image.shape) == 3 else image\n",
    "        gray = (gray * 255).astype(np.uint8)\n",
    "        distances = [1, 2, 3]\n",
    "        angles = [0, 45, 90, 135]\n",
    "        properties = ['contrast', 'correlation', 'homogeneity', 'energy']\n",
    "        features = []\n",
    "        for distance in distances:\n",
    "            for angle in angles:\n",
    "                glcm = feature.graycomatrix(gray, [distance], [np.radians(angle)],\n",
    "                                            levels=256, symmetric=True, normed=True)\n",
    "                for prop in properties:\n",
    "                    features.append(feature.graycoprops(glcm, prop)[0, 0])\n",
    "        return np.array(features)\n",
    "\n",
    "    def extract_lbp_features(self, image):\n",
    "        gray = rgb2gray(image) if len(image.shape) == 3 else image\n",
    "        gray = (gray * 255).astype(np.uint8)\n",
    "        radius = 3\n",
    "        n_points = 8 * radius\n",
    "        lbp = feature.local_binary_pattern(gray, n_points, radius, method='uniform')\n",
    "        hist, _ = np.histogram(lbp.ravel(), bins=n_points + 2,\n",
    "                               range=(0, n_points + 2), density=True)\n",
    "        return hist\n",
    "\n",
    "    def extract_gabor_features(self, image):\n",
    "        gray = rgb2gray(image) if len(image.shape) == 3 else image\n",
    "        features = []\n",
    "        frequencies = [0.1, 0.3, 0.5]\n",
    "        angles = [0, 45, 90, 135]\n",
    "        for freq in frequencies:\n",
    "            for angle in angles:\n",
    "                filt_real, _ = gabor(gray, frequency=freq, theta=np.radians(angle))\n",
    "                features.extend([\n",
    "                    np.mean(filt_real),\n",
    "                    np.var(filt_real),\n",
    "                    np.std(filt_real)\n",
    "                ])\n",
    "        return np.array(features)\n",
    "\n",
    "    def extract_color_features(self, image):\n",
    "        if len(image.shape) == 3:\n",
    "            features = []\n",
    "            for channel in range(3):\n",
    "                channel_data = image[:, :, channel].flatten()\n",
    "                features.extend([\n",
    "                    np.mean(channel_data),\n",
    "                    np.std(channel_data),\n",
    "                    np.mean((channel_data - np.mean(channel_data)) ** 3)\n",
    "                ])\n",
    "            for channel in range(3):\n",
    "                hist, _ = np.histogram(image[:, :, channel], bins=32, range=(0, 1))\n",
    "                features.extend(hist / np.sum(hist))\n",
    "            return np.array(features)\n",
    "        else:\n",
    "            gray_data = image.flatten()\n",
    "            features = [\n",
    "                np.mean(gray_data),\n",
    "                np.std(gray_data),\n",
    "                np.mean((gray_data - np.mean(gray_data)) ** 3)\n",
    "            ]\n",
    "            hist, _ = np.histogram(image, bins=32, range=(0, 1))\n",
    "            features.extend(hist / np.sum(hist))\n",
    "            return np.array(features)\n",
    "\n",
    "    def extract_all_features(self, image):\n",
    "        glcm_features = self.extract_glcm_features(image)\n",
    "        lbp_features = self.extract_lbp_features(image)\n",
    "        gabor_features = self.extract_gabor_features(image)\n",
    "        color_features = self.extract_color_features(image)\n",
    "        return np.concatenate([glcm_features, lbp_features, gabor_features, color_features])\n",
    "\n",
    "# Fungsi untuk memuat dan memproses data\n",
    "def load_and_process_data(data_dir, feature_extractor, method_name):\n",
    "    print(f\"\\nMemproses data dengan metode: {method_name}\")\n",
    "    X = []\n",
    "    y = []\n",
    "    class_names = os.listdir(data_dir)\n",
    "    class_names = [cls for cls in class_names if os.path.isdir(os.path.join(data_dir, cls))]\n",
    "    print(f\"Kelas yang ditemukan: {class_names}\")\n",
    "\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        image_files = [f for f in os.listdir(class_dir)\n",
    "                       if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        print(f\"Memproses kelas {class_name}: {len(image_files)} gambar\")\n",
    "\n",
    "        for i, img_file in enumerate(image_files, 1):\n",
    "            img_path = os.path.join(class_dir, img_file)\n",
    "            if i % 50 == 0 or i == 1:\n",
    "                print(f\"Memproses gambar {i}/{len(image_files)} di kelas {class_name}\")\n",
    "            try:\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    print(f\"Gagal membaca gambar: {img_path}\")\n",
    "                    continue\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "                image = image.astype(np.float32) / 255.0\n",
    "                if method_name == \"Texture Features\":\n",
    "                    features = feature_extractor.extract_all_features(image)\n",
    "                X.append(features)\n",
    "                y.append(class_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Total gambar yang diproses: {len(X)}, Distribusi kelas: {Counter(y)}\")\n",
    "    return np.array(X), np.array(y), class_names\n",
    "\n",
    "# Fungsi untuk melatih model\n",
    "def train_and_evaluate_models(X, y, class_names, method_name):\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "        'SVM': SVC(kernel='rbf', random_state=RANDOM_STATE),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
    "    }\n",
    "\n",
    "    # Buat direktori TEST1 jika belum ada\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = {}\n",
    "    print(f\"\\n=== Pelatihan Model dengan {method_name} ===\")\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nMelatih {model_name}...\")\n",
    "        # Validasi silang untuk deteksi overfitting\n",
    "        cv_scores = cross_val_score(model, X_scaled, y_encoded, cv=5, scoring='accuracy')\n",
    "        print(f\"Cross-validation scores for {model_name}: {cv_scores}\")\n",
    "        print(f\"Mean CV accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "        model.fit(X_scaled, y_encoded)\n",
    "        # Simpan model di TEST1\n",
    "        model_path = os.path.join(output_dir, f\"{model_name.replace(' ', '_')}_model.pkl\")\n",
    "        joblib.dump(model, model_path)\n",
    "        print(f\"Model disimpan di: {model_path}\")\n",
    "\n",
    "    # Simpan scaler dan label encoder di TEST1\n",
    "    scaler_path = os.path.join(output_dir, \"scaler.pkl\")\n",
    "    le_path = os.path.join(output_dir, \"label_encoder.pkl\")\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    joblib.dump(le, le_path)\n",
    "    print(f\"Scaler disimpan di: {scaler_path}\")\n",
    "    print(f\"Label encoder disimpan di: {le_path}\")\n",
    "\n",
    "    return results, scaler, le\n",
    "\n",
    "# Fungsi untuk evaluasi pada dataset test\n",
    "def evaluate_test_set(test_dir, feature_extractor, models, scaler, le, method_name):\n",
    "    print(f\"\\n=== Evaluasi pada Dataset Test dengan {method_name} ===\")\n",
    "    X_test, y_test, class_names = load_and_process_data(test_dir, feature_extractor, method_name)\n",
    "    if len(X_test) == 0:\n",
    "        print(\"Error: Tidak ada data test yang diproses!\")\n",
    "        return\n",
    "\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluasi {model_name} pada dataset test...\")\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "        print(f\"{model_name} Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"\\nClassification Report for {model_name} (Test Set):\")\n",
    "        print(classification_report(y_test_encoded, y_pred, target_names=class_names))\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        cm = confusion_matrix(y_test_encoded, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f'Confusion Matrix - {model_name} (Test Set) with {method_name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Periksa direktori train\n",
    "    if not os.path.exists(train_path):\n",
    "        print(f\"Error: Direktori {train_path} tidak ditemukan!\")\n",
    "        return\n",
    "\n",
    "    print(\"Struktur direktori train:\")\n",
    "    for root, dirs, files in os.walk(train_path, topdown=True):\n",
    "        print(f\"Direktori: {root}\")\n",
    "        for d in dirs:\n",
    "            print(f\"  └── {d} ({len(os.listdir(os.path.join(root, d)))} files)\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"METODE 1: TEXTURE FEATURES (GLCM + LBP + Gabor + Color)\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        texture_extractor = TextureFeatureExtractor()\n",
    "        X_texture, y_texture, class_names = load_and_process_data(\n",
    "            train_path, texture_extractor, \"Texture Features\"\n",
    "        )\n",
    "\n",
    "        if len(X_texture) > 0:\n",
    "            print(f\"Total gambar: {len(X_texture)}, Distribusi kelas: {Counter(y_texture)}\")\n",
    "            results_texture, scaler_texture, le_texture = train_and_evaluate_models(\n",
    "                X_texture, y_texture, class_names, \"Texture Features\"\n",
    "            )\n",
    "\n",
    "            # Evaluasi pada dataset test\n",
    "            if os.path.exists(test_path):\n",
    "                models = {\n",
    "                    'Random Forest': joblib.load(os.path.join(output_dir, \"Random_Forest_model.pkl\")),\n",
    "                    'SVM': joblib.load(os.path.join(output_dir, \"SVM_model.pkl\")),\n",
    "                    'K-Nearest Neighbors': joblib.load(os.path.join(output_dir, \"KNeighbors_model.pkl\"))\n",
    "                }\n",
    "                evaluate_test_set(test_path, texture_extractor, models, scaler_texture, le_texture, \"Texture Features\")\n",
    "            else:\n",
    "                print(f\"Direktori test {test_path} tidak ditemukan. Lewati evaluasi test.\")\n",
    "\n",
    "        else:\n",
    "            print(\"Error: Tidak ada data yang diproses. Periksa struktur dataset.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nPastikan dataset memiliki struktur direktori yang benar:\")\n",
    "        print(\"train/\")\n",
    "        print(\"├── 001/\")\n",
    "        print(\"│   ├── img1.png\")\n",
    "        print(\"│   └── ...\")\n",
    "        print(\"├── 002/\")\n",
    "        print(\"└── ...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52771d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
